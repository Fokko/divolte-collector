//
// Copyright 2014 GoDataDriven B.V.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

divolte {
  server {
    host = 0.0.0.0
    host = ${?DIVOLTE_HOST}

    port = 8290
    port = ${?DIVOLTE_PORT}

    use_x_forwarded_for = false

    serve_static_resources = true
  }

  tracking {
    party_cookie = _dvp
    party_timeout = 730 days
    session_cookie = _dvs
    session_timeout = 30 minutes
    // cookie_domain = ''
  }

  javascript {
    name = divolte.js
    logging = false
    debug = false
  }

  incoming_request_processor {
    threads = 2
    max_write_queue = 100000
    max_enqueue_delay = 1 second

    discard_corrupted = false

    duplicate_memory_size = 1000000
    discard_duplicates = false
  }

  kafka_flusher {
    enabled = false

    max_write_queue = 200000
    max_enqueue_delay = 1 second
    threads = 2

    // The topic onto which events are published.
    topic = "divolte"
    topic = ${?DIVOLTE_KAFKA_TOPIC}

    // All settings in here are used as-is to configure
    // the Kafka producer.
    // See: http://kafka.apache.org/documentation.html#producerconfigs
    producer = {
      metadata.broker.list = ["localhost:9092"]
      metadata.broker.list = ${?DIVOLTE_KAFKA_BROKER_LIST}

      client.id = divolte.collector
      client.id = ${?DIVOLTE_KAFKA_CLIENT_ID}

      request.required.acks = 0
      message.send.max.retries = 5
      retry.backoff.ms = 200
    }
  }

  hdfs_flusher {
    enabled = true

    max_write_queue = 100000
    max_enqueue_delay = 1 second
    threads = 2

    hdfs {
      // default nonexistant: Use HADOOP_CONF_DIR on the classpath.
      // If not present empty config results in local filesystem being used.
      // uri = "file:///"
      // uri = ${?DIVOLTE_HDFS_URI}
      // defaults to 1
      replication = 1
      replication = ${?DIVOLTE_HDFS_REPLICATION}
    }

    simple_rolling_file_strategy {
      roll_every = 60 minutes
      sync_file_after_records = 1000
      sync_file_after_duration = 30 seconds

      // defaults to /tmp
      working_dir = /tmp
      publish_dir = /tmp
    }
  }

  tracking.ua_parser {
    type = non_updating
    cache_size = 1000
  }
}
